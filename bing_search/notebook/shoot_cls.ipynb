{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fusemix/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "# processor = AutoImageProcessor.from_pretrained(\"al-css/Screenshots_detection_to_classification\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"al-css/Screenshots_detection_to_classification\")\n",
    "processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"fxmarty/clip-vision-model-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model.eval()\n",
    "model.to(torch.float16)\n",
    "image_path = '/workspace/repo/llm2clip/EVA-CLIP/tmp/bing_images/0_4_7.jpeg'\n",
    "# convert to tensor\n",
    "image = Image.open(image_path)\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "# PIL.Image.Image\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 128, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 128)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=128, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-4.0352,  4.3125]], dtype=torch.float16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fusemix/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/fusemix/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"yujiepan/clip-vit-tiny-random-patch14-336\"\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0_4_77.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1386,  1.4008, -0.2838,  4.6064]])\n",
      "Image: 0_4_86.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.3969, -1.7005, -2.6309,  1.5089]])\n",
      "Image: 0_4_4.jpeg, Prediction: Screenshot, Probability: tensor([[ 1.1040, -1.6138, -2.5328,  0.3238]])\n",
      "Image: 0_4_15.jpeg, Prediction: Graphic, Probability: tensor([[ 3.8941,  1.2394, -0.3086,  4.3425]])\n",
      "Image: 0_4_73.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1113,  1.6057, -0.0648,  4.6256]])\n",
      "Image: 0_4_63.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1580,  1.5433, -0.0778,  4.6855]])\n",
      "Image: 0_4_85.jpeg, Prediction: Screenshot, Probability: tensor([[ 1.1240, -1.5760, -2.5020,  0.3503]])\n",
      "Image: 0_4_5.jpeg, Prediction: Graphic, Probability: tensor([[ 3.9393,  1.3604, -0.2450,  4.3652]])\n",
      "Image: 0_4_22.jpeg, Prediction: Graphic, Probability: tensor([[ 3.8994,  1.0342, -0.5647,  4.2048]])\n",
      "Image: 0_4_68.jpeg, Prediction: Graphic, Probability: tensor([[4.2290, 1.7465, 0.1091, 4.8476]])\n",
      "Image: 0_4_76.jpeg, Prediction: Graphic, Probability: tensor([[4.1973, 1.6524, 0.0226, 4.7480]])\n",
      "Image: 0_4_14.jpeg, Prediction: Graphic, Probability: tensor([[ 3.9393,  1.3604, -0.2450,  4.3652]])\n",
      "Image: 0_4_61.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0741,  1.5738, -0.0595,  4.6296]])\n",
      "Image: 0_4_9.jpeg, Prediction: Screenshot, Probability: tensor([[ 1.1545, -1.4314, -2.3656,  0.4410]])\n",
      "Image: 0_4_69.jpeg, Prediction: Screenshot, Probability: tensor([[-0.7143, -2.6083, -3.0475, -2.0664]])\n",
      "Image: 0_4_67.jpeg, Prediction: Screenshot, Probability: tensor([[ 0.7166, -1.6114, -2.5005, -0.2123]])\n",
      "Image: 0_4_88.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1113,  1.6057, -0.0648,  4.6256]])\n",
      "Image: 0_4_40.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0713,  1.4309, -0.1712,  4.5596]])\n",
      "Image: 0_4_19.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.4362,  0.2548, -1.1559,  2.3609]])\n",
      "Image: 0_4_46.jpeg, Prediction: Screenshot, Probability: tensor([[ 1.1240, -1.5760, -2.5020,  0.3503]])\n",
      "Image: 0_4_23.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.3969, -1.7005, -2.6309,  1.5089]])\n",
      "Image: 0_4_31.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0713,  1.4309, -0.1712,  4.5596]])\n",
      "Image: 0_4_1.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1027,  1.5769, -0.1048,  4.5983]])\n",
      "Image: 0_4_26.jpeg, Prediction: Graphic, Probability: tensor([[4.2238, 1.7577, 0.1073, 4.8339]])\n",
      "Image: 0_4_13.jpeg, Prediction: Graphic, Probability: tensor([[4.1973, 1.6524, 0.0226, 4.7480]])\n",
      "Image: 0_4_51.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0741,  1.5738, -0.0595,  4.6296]])\n",
      "Image: 0_4_79.jpeg, Prediction: Screenshot, Probability: tensor([[-0.7143, -2.6083, -3.0475, -2.0664]])\n",
      "Image: 0_4_66.jpeg, Prediction: Screenshot, Probability: tensor([[ 0.3962, -2.5946, -2.9868, -0.7018]])\n",
      "Image: 0_4_75.jpeg, Prediction: Graphic, Probability: tensor([[4.1665e+00, 1.6294e+00, 3.3858e-03, 4.7251e+00]])\n",
      "Image: 0_4_74.jpeg, Prediction: Graphic, Probability: tensor([[4.2238, 1.7577, 0.1073, 4.8339]])\n",
      "Image: 0_4_42.jpeg, Prediction: Graphic, Probability: tensor([[4.2290, 1.7465, 0.1091, 4.8476]])\n",
      "Image: 0_4_24.jpeg, Prediction: Graphic, Probability: tensor([[4.2240, 1.7478, 0.1129, 4.8439]])\n",
      "Image: 0_4_80.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.4362,  0.2548, -1.1559,  2.3609]])\n",
      "Image: 0_4_41.jpeg, Prediction: Graphic, Probability: tensor([[ 2.9534,  1.0990, -0.4734,  3.3071]])\n",
      "Image: 0_4_84.jpeg, Prediction: Graphic, Probability: tensor([[4.2188, 1.6853, 0.0568, 4.8142]])\n",
      "Image: 0_4_55.jpeg, Prediction: Screenshot, Probability: tensor([[ 0.7166, -1.6114, -2.5005, -0.2123]])\n",
      "Image: 0_4_25.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1558,  1.5537, -0.0591,  4.6998]])\n",
      "Image: 0_4_49.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.4385, -0.0029, -1.4169,  2.2696]])\n",
      "Image: 0_4_21.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0352,  1.3022, -0.4084,  4.4421]])\n",
      "Image: 0_4_29.jpeg, Prediction: Graphic, Probability: tensor([[ 4.0352,  1.3022, -0.4084,  4.4421]])\n",
      "Image: 0_4_12.jpeg, Prediction: Screenshot, Probability: tensor([[ 2.2299, -1.4687, -2.5330,  1.3995]])\n",
      "Image: 0_4_18.jpeg, Prediction: Graphic, Probability: tensor([[ 3.8994,  1.0342, -0.5647,  4.2048]])\n",
      "Image: 0_4_16.jpeg, Prediction: Graphic, Probability: tensor([[4.2240, 1.7478, 0.1129, 4.8439]])\n",
      "Image: 0_4_27.jpeg, Prediction: Screenshot, Probability: tensor([[ 0.3962, -2.5946, -2.9868, -0.7018]])\n",
      "Image: 0_4_0.jpeg, Prediction: Graphic, Probability: tensor([[4.2446, 1.6915, 0.0316, 4.7998]])\n",
      "Image: 0_4_72.jpeg, Prediction: Graphic, Probability: tensor([[ 4.1580,  1.5433, -0.0778,  4.6855]])\n",
      "Image: 0_4_62.jpeg, Prediction: Screenshot, Probability: tensor([[-0.7543, -2.7089, -3.1726, -2.2155]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_roots = '/workspace/repo/llm2clip/EVA-CLIP/tmp/bing_images/TeamViewer/TeamViewer-session-list'\n",
    "\n",
    "image_paths = os.listdir(image_roots)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor image_path in image_paths:\n",
    "\t\timage = Image.open(os.path.join(image_roots, image_path))\n",
    "\t\ttexts = [\"Screenshot\", \"photograph\",\" Artwork\",\"Graphic\"]\n",
    "\t\tinputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\t\toutputs = model(**inputs)\n",
    "\t\tlogits_per_image = outputs.logits_per_image\n",
    "\t\tprobs = logits_per_image\n",
    "\t\tresult_index = torch.argmax(probs)\n",
    "\t\tprint(f\"Image: {image_path}, Prediction: {texts[result_index]}, Probability: {probs}\")\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0_4_4.jpeg, flag:0, Probability: tensor([[ 1.2763, -1.6131]])\n",
      "Image: 0_4_85.jpeg, flag:0, Probability: tensor([[ 1.2627, -1.6765]])\n",
      "Image: 0_4_68.jpeg, flag:0, Probability: tensor([[ 0.0630, -0.6162]])\n",
      "Image: 0_4_9.jpeg, flag:0, Probability: tensor([[ 2.3956, -2.9959]])\n",
      "Image: 0_4_69.jpeg, flag:0, Probability: tensor([[ 1.7534, -0.6616]])\n",
      "Image: 0_4_67.jpeg, flag:0, Probability: tensor([[ 1.7777, -1.6568]])\n",
      "Image: 0_4_46.jpeg, flag:0, Probability: tensor([[ 1.2627, -1.6765]])\n",
      "Image: 0_4_79.jpeg, flag:0, Probability: tensor([[ 1.7534, -0.6616]])\n",
      "Image: 0_4_66.jpeg, flag:0, Probability: tensor([[ 2.6095, -1.6284]])\n",
      "Image: 0_4_42.jpeg, flag:0, Probability: tensor([[ 0.0630, -0.6162]])\n",
      "Image: 0_4_55.jpeg, flag:0, Probability: tensor([[ 1.7777, -1.6568]])\n",
      "Image: 0_4_27.jpeg, flag:0, Probability: tensor([[ 2.6095, -1.6284]])\n",
      "Image: 0_4_62.jpeg, flag:0, Probability: tensor([[ 0.6416, -0.6572]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_roots = '/workspace/repo/llm2clip/EVA-CLIP/tmp/bing_images/TeamViewer/TeamViewer-session-list'\n",
    "\n",
    "image_paths = os.listdir(image_roots)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor image_path in image_paths:\n",
    "\t\timage = Image.open(os.path.join(image_roots, image_path))\n",
    "\t\tinputs = processor(images=image, return_tensors=\"pt\")\n",
    "\t\toutputs = model(**inputs)\n",
    "\t\tprobs = outputs[-1]\n",
    "\t\tflag = probs.argmax().item()\n",
    "\t\tif flag == 0:\n",
    "\t\t\tprint(f\"Image: {image_path}, flag:{flag}, Probability: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from PIL import Image\n",
    "\n",
    "target_cwd = '/workspace/repo/cccrawl'\n",
    "sys.path.append(target_cwd)\n",
    "\n",
    "from bing_search.models import ScreenshotChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_checker = ScreenshotChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_roots = '/workspace/repo/llm2clip/EVA-CLIP/tmp/bing_images_any/Excel-chart-tools'\n",
    "\n",
    "image_paths = os.listdir(image_roots)\n",
    "\n",
    "for image_path in image_paths:\n",
    "\timage = Image.open(os.path.join(image_roots, image_path))\n",
    "\tflag = screenshot_checker.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8107829093933105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(flag[0].norm(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Adam-Fourney-2/publication/316560440/figure/download/fig13/AS:488373489016840@1493448722310/Screenshot-of-the-Microsoft-Word-Online-user-interface-37-a-web-based.png",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 25\u001b[0m\n\u001b[1;32m     15\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 'Accept-Language': 'en-US,en;q=0.9',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 'Upgrade-Insecure-Requests': '1'\u001b[39;00m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(content_url, headers\u001b[38;5;241m=\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#发现出现很多\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[1;32m     28\u001b[0m image\n",
      "File \u001b[0;32m/opt/conda/envs/omni/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Adam-Fourney-2/publication/316560440/figure/download/fig13/AS:488373489016840@1493448722310/Screenshot-of-the-Microsoft-Word-Online-user-interface-37-a-web-based.png"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "content_url = 'https://www.screenshotonpc.com/wp-content/uploads/2020/06/screenshot-word-click-screenshot-microsoft-3.png'\n",
    "\n",
    "content_url = 'https://www.screenshotonpc.com/wp-content/uploads/2020/06/microsoft-word-featured-image-1.png'\n",
    "\n",
    "content_url = \"http://www.goskills.com/Video/1233/thumbnail-720p.jpg?t=T51444\"\n",
    "\n",
    "\n",
    "content_url = \"https://www.researchgate.net/profile/Adam-Fourney-2/publication/316560440/figure/download/fig13/AS:488373489016840@1493448722310/Screenshot-of-the-Microsoft-Word-Online-user-interface-37-a-web-based.png\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    # 'Accept-Language': 'en-US,en;q=0.9',\n",
    "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    # 'Connection': 'close',\n",
    "    # 'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "response = requests.get(content_url, headers=headers, stream=True) #发现出现很多\n",
    "response.raise_for_status()\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
